#!/usr/bin/env python3
"""
10M Parameter Python Code Model - Pipeline Validation
======================================================
A complete JAX/Flax training pipeline for validating the architecture
before scaling to 500M parameters.

Target: Kaggle TPU v5e-8, Google Colab TPU, or local CPU/GPU testing

Usage:
    python train_10m.py                    # Run full pipeline
    python train_10m.py --validate-only    # Only run validation checks
    python train_10m.py --steps 100        # Quick test with fewer steps
"""

# ============ IMPORTS ============
import os
import ast
import re
import sys
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, List, Iterator, Dict, Any
from functools import partial

import numpy as np

# JAX imports
import jax
import jax.numpy as jnp
from jax import random as jrandom

# Flax imports
import flax
from flax import linen as nn
from flax.training import train_state

# Optax for optimization
import optax

# HuggingFace imports
from transformers import AutoTokenizer
from datasets import load_dataset

# Progress bar
from tqdm import tqdm

# Set random seeds for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)


# ============ CONFIGURATION ============
@dataclass
class ModelConfig:
    """Model architecture configuration for 10M parameter model."""
    vocab_size: int = 49152      # StarCoder2 tokenizer
    hidden_dim: int = 256        # Embedding dimension
    num_layers: int = 6          # Transformer blocks
    num_heads: int = 4           # Attention heads (for Q)
    head_dim: int = 64           # Dimension per head
    ffn_dim: int = 1024          # Feed-forward dimension (4x hidden)
    max_seq_len: int = 512       # Maximum sequence length
    dropout_rate: float = 0.0    # No dropout for small model
    rope_theta: float = 10000.0  # RoPE base frequency


@dataclass
class TrainingConfig:
    """Training hyperparameters."""
    learning_rate: float = 1e-3
    warmup_steps: int = 100
    total_steps: int = 5000
    weight_decay: float = 0.1
    grad_clip: float = 1.0
    beta1: float = 0.9
    beta2: float = 0.95
    log_every: int = 100
    save_every: int = 1000


@dataclass
class DataConfig:
    """Data loading configuration."""
    # Target tokens based on Chinchilla scaling (~20x params for 10M model)
    target_tokens: int = 200_000_000  # 200M tokens
    min_file_chars: int = 200
    max_file_chars: int = 50_000
    streaming: bool = True


# Detect device and set appropriate settings
def detect_device():
    """Detect available device and return appropriate settings."""
    devices = jax.devices()
    device = devices[0]

    if device.platform == 'tpu':
        return 'TPU', 64, jnp.bfloat16
    elif device.platform == 'gpu':
        return 'GPU', 16, jnp.bfloat16
    else:
        return 'CPU', 4, jnp.float32


DEVICE_TYPE, BATCH_SIZE, DTYPE = detect_device()


# ============ DATA QUALITY SYSTEM ============
class StrictPythonFilter:
    """
    STRICT quality filtering for Python code.
    Only high-quality, well-documented code passes through.
    """

    # Patterns indicating auto-generated code
    AUTOGEN_PATTERNS = [
        r'generated\s+by',
        r'auto[-_]?generated',
        r'do\s+not\s+edit',
        r'this\s+file\s+is\s+generated',
        r'automatically\s+generated',
        r'machine\s+generated',
    ]

    def __init__(self, config: DataConfig):
        self.config = config
        self.autogen_regex = re.compile(
            '|'.join(self.AUTOGEN_PATTERNS),
            re.IGNORECASE
        )

    def is_valid_syntax(self, code: str) -> Tuple[bool, Optional[ast.AST]]:
        """Check if code parses as valid Python. Returns (valid, AST)."""
        try:
            tree = ast.parse(code)
            return True, tree
        except SyntaxError:
            return False, None
        except Exception:
            return False, None

    def passes_size_filter(self, code: str) -> bool:
        """Check if code is within acceptable size range."""
        length = len(code)
        return self.config.min_file_chars < length < self.config.max_file_chars

    def is_not_autogenerated(self, code: str) -> bool:
        """Check if code is NOT auto-generated."""
        # Only check first 1000 chars for efficiency
        header = code[:1000].lower()
        return self.autogen_regex.search(header) is None

    def is_not_minified(self, code: str) -> bool:
        """Check if code is NOT minified (avg line length < 150)."""
        lines = code.split('\n')
        if not lines:
            return False
        avg_length = sum(len(line) for line in lines) / len(lines)
        return avg_length < 150

    def has_proper_structure(self, tree: ast.AST) -> bool:
        """Check if code has at least one function or class definition."""
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                return True
        return False

    def has_documentation(self, tree: ast.AST) -> bool:
        """Check if code has docstrings OR type hints."""
        has_docstring = False
        has_type_hint = False

        for node in ast.walk(tree):
            # Check for docstrings
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module)):
                if (node.body and isinstance(node.body[0], ast.Expr) and
                    isinstance(node.body[0].value, ast.Constant) and
                    isinstance(node.body[0].value.value, str)):
                    has_docstring = True

            # Check for type hints
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if node.returns is not None:
                    has_type_hint = True
                for arg in node.args.args:
                    if arg.annotation is not None:
                        has_type_hint = True

            if isinstance(node, ast.AnnAssign):
                has_type_hint = True

        return has_docstring or has_type_hint

    def is_not_data_heavy(self, code: str) -> bool:
        """Check if code is NOT mostly string literals or comments."""
        lines = code.split('\n')
        if not lines:
            return False

        string_comment_lines = 0
        for line in lines:
            stripped = line.strip()
            # Comment line
            if stripped.startswith('#'):
                string_comment_lines += 1
            # String-heavy line (rough heuristic)
            elif stripped.count('"') > 4 or stripped.count("'") > 4:
                string_comment_lines += 1

        ratio = string_comment_lines / len(lines)
        return ratio < 0.3  # Less than 30% comments/strings

    def passes_all_filters(self, code: str) -> Tuple[bool, float, Optional[ast.AST]]:
        """
        Apply all strict filters.
        Returns (passes, quality_score, AST).
        """
        # Filter 1: Size
        if not self.passes_size_filter(code):
            return False, 0.0, None

        # Filter 2: Valid syntax
        valid, tree = self.is_valid_syntax(code)
        if not valid:
            return False, 0.0, None

        # Filter 3: Not auto-generated
        if not self.is_not_autogenerated(code):
            return False, 0.0, None

        # Filter 4: Not minified
        if not self.is_not_minified(code):
            return False, 0.0, None

        # Filter 5: Has proper structure (functions/classes)
        if not self.has_proper_structure(tree):
            return False, 0.0, None

        # Filter 6: Has documentation (docstrings or type hints)
        if not self.has_documentation(tree):
            return False, 0.0, None

        # Filter 7: Not data-heavy
        if not self.is_not_data_heavy(code):
            return False, 0.0, None

        # All filters passed - compute quality score
        score = self.compute_quality_score(code, tree)
        return True, score, tree

    def compute_quality_score(self, code: str, tree: ast.AST) -> float:
        """
        Compute quality score for sorting (higher = better).
        Only called on code that passes all filters.
        """
        score = 0.0

        # Count docstrings (0-3 points)
        docstring_count = 0
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                if (node.body and isinstance(node.body[0], ast.Expr) and
                    isinstance(node.body[0].value, ast.Constant) and
                    isinstance(node.body[0].value.value, str)):
                    docstring_count += 1
        score += min(docstring_count * 0.5, 3.0)

        # Count type annotations (0-3 points)
        type_hint_count = 0
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if node.returns is not None:
                    type_hint_count += 1
                type_hint_count += sum(1 for arg in node.args.args if arg.annotation)
            if isinstance(node, ast.AnnAssign):
                type_hint_count += 1
        score += min(type_hint_count * 0.3, 3.0)

        # Code structure (0-2 points)
        class_count = sum(1 for n in ast.walk(tree) if isinstance(n, ast.ClassDef))
        func_count = sum(1 for n in ast.walk(tree) if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef)))
        score += min(class_count * 0.5 + func_count * 0.2, 2.0)

        # Complexity balance (0-2 points)
        lines = len(code.split('\n'))
        if 20 <= lines <= 300:
            score += 2.0
        elif 10 <= lines <= 500:
            score += 1.0

        return score


class QualityDataLoader:
    """
    Load Python code from The Stack v2 with strict quality filtering.
    Data is sorted by quality score (highest first).
    """

    def __init__(self, tokenizer, config: DataConfig, model_config: ModelConfig):
        self.tokenizer = tokenizer
        self.config = config
        self.model_config = model_config
        self.filter = StrictPythonFilter(config)
        self.pad_token_id = tokenizer.pad_token_id

    def load_dataset_streaming(self) -> Iterator:
        """Load The Stack v2 Python subset with streaming."""
        try:
            # Try The Stack v2 first
            print("Loading bigcode/starcoderdata (Python subset)...")
            dataset = load_dataset(
                "bigcode/starcoderdata",
                data_dir="python",
                split="train",
                streaming=True,
                trust_remote_code=True
            )
            return iter(dataset)
        except Exception as e:
            print(f"Could not load starcoderdata: {e}")
            try:
                # Fallback to codeparrot-clean
                print("Falling back to codeparrot/codeparrot-clean...")
                dataset = load_dataset(
                    "codeparrot/codeparrot-clean",
                    split="train",
                    streaming=True
                )
                return iter(dataset)
            except Exception as e2:
                print(f"Could not load codeparrot-clean: {e2}")
                raise RuntimeError("Could not load any dataset. Please check HuggingFace access.")

    def filter_and_score_batch(
        self,
        examples: List[Dict],
        max_samples: int = 10000
    ) -> List[Tuple[str, float]]:
        """
        Filter a batch of examples and return (code, score) pairs.
        Sorted by score descending.
        """
        results = []
        total_checked = 0
        passed = 0

        for example in examples:
            # Get content field (varies by dataset)
            content = example.get('content') or example.get('code') or example.get('text', '')

            if not content:
                continue

            total_checked += 1
            passes, score, _ = self.filter.passes_all_filters(content)

            if passes:
                results.append((content, score))
                passed += 1

                if len(results) >= max_samples:
                    break

        # Sort by quality score (highest first)
        results.sort(key=lambda x: x[1], reverse=True)

        print(f"Quality filter: {passed}/{total_checked} passed ({100*passed/max(total_checked,1):.1f}%)")
        return results

    def tokenize_samples(
        self,
        samples: List[Tuple[str, float]]
    ) -> List[np.ndarray]:
        """Tokenize samples and return list of token arrays."""
        tokenized = []

        for code, score in samples:
            tokens = self.tokenizer.encode(
                code,
                truncation=True,
                max_length=self.model_config.max_seq_len + 1,
                add_special_tokens=True
            )

            if len(tokens) > 10:  # Skip very short sequences
                tokenized.append(np.array(tokens, dtype=np.int32))

        return tokenized

    def create_batches(
        self,
        tokenized_samples: List[np.ndarray],
        batch_size: int
    ) -> Iterator[np.ndarray]:
        """Create batches with padding."""
        seq_len = self.model_config.max_seq_len + 1

        # Shuffle samples
        indices = list(range(len(tokenized_samples)))
        random.shuffle(indices)

        batch = []
        for idx in indices:
            tokens = tokenized_samples[idx]

            # Pad or truncate to seq_len
            if len(tokens) < seq_len:
                padded = np.full(seq_len, self.pad_token_id, dtype=np.int32)
                padded[:len(tokens)] = tokens
            else:
                padded = tokens[:seq_len]

            batch.append(padded)

            if len(batch) == batch_size:
                yield np.stack(batch)
                batch = []

        # Yield remaining samples (if any)
        if batch:
            # Pad to full batch size
            while len(batch) < batch_size:
                batch.append(np.full(seq_len, self.pad_token_id, dtype=np.int32))
            yield np.stack(batch)

    def get_training_data(
        self,
        num_samples: int = 50000,
        batch_size: int = None
    ) -> Tuple[List[np.ndarray], Dict[str, Any]]:
        """
        Load, filter, score, and prepare training data.
        Returns (tokenized_samples, stats).
        """
        if batch_size is None:
            batch_size = BATCH_SIZE

        print(f"\n{'='*50}")
        print("Loading and filtering training data...")
        print(f"{'='*50}")

        # Load dataset
        dataset_iter = self.load_dataset_streaming()

        # Collect examples
        print(f"Collecting up to {num_samples * 5} examples for filtering...")
        examples = []
        for i, example in enumerate(dataset_iter):
            examples.append(example)
            if i >= num_samples * 5:  # Collect 5x more than needed for filtering
                break
            if (i + 1) % 10000 == 0:
                print(f"  Collected {i+1} examples...")

        # Filter and score
        print(f"\nApplying STRICT quality filters...")
        scored_samples = self.filter_and_score_batch(examples, max_samples=num_samples)

        # Print quality distribution
        if scored_samples:
            scores = [s for _, s in scored_samples]
            print(f"\nQuality score distribution:")
            print(f"  Top score: {max(scores):.2f}")
            print(f"  Avg score: {sum(scores)/len(scores):.2f}")
            print(f"  Min score: {min(scores):.2f}")

        # Tokenize
        print(f"\nTokenizing {len(scored_samples)} high-quality samples...")
        tokenized = self.tokenize_samples(scored_samples)

        # Compute stats
        total_tokens = sum(len(t) for t in tokenized)
        stats = {
            'num_samples': len(tokenized),
            'total_tokens': total_tokens,
            'avg_length': total_tokens / max(len(tokenized), 1),
            'batch_size': batch_size,
            'num_batches': len(tokenized) // batch_size
        }

        print(f"\nData prepared:")
        print(f"  Samples: {stats['num_samples']:,}")
        print(f"  Total tokens: {stats['total_tokens']:,}")
        print(f"  Avg sequence length: {stats['avg_length']:.1f}")
        print(f"  Batches: {stats['num_batches']}")

        return tokenized, stats


# ============ MODEL COMPONENTS ============
class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization."""
    dim: int
    eps: float = 1e-6
    dtype: Any = jnp.float32

    @nn.compact
    def __call__(self, x):
        # Cast to float32 for numerical stability
        x_f32 = x.astype(jnp.float32)

        # RMS normalization
        variance = jnp.mean(x_f32 ** 2, axis=-1, keepdims=True)
        x_normed = x_f32 * jax.lax.rsqrt(variance + self.eps)

        # Learnable scale parameter
        scale = self.param('scale', nn.initializers.ones, (self.dim,))

        return (x_normed * scale).astype(self.dtype)


def create_rope_embeddings(dim: int, max_seq_len: int, theta: float = 10000.0):
    """Create rotary position embedding frequencies."""
    # Compute inverse frequencies
    inv_freq = 1.0 / (theta ** (np.arange(0, dim, 2).astype(np.float32) / dim))

    # Position indices
    positions = np.arange(max_seq_len).astype(np.float32)

    # Outer product: [seq_len, dim/2]
    freqs = np.outer(positions, inv_freq)

    # Create sin/cos embeddings
    cos_emb = np.cos(freqs)
    sin_emb = np.sin(freqs)

    return jnp.array(cos_emb), jnp.array(sin_emb)


def apply_rope(x, cos, sin):
    """
    Apply rotary position embeddings to input tensor.
    x: [batch, seq_len, num_heads, head_dim]
    """
    # Split into pairs for rotation
    x1 = x[..., ::2]   # Even indices
    x2 = x[..., 1::2]  # Odd indices

    # Get sequence length
    seq_len = x.shape[1]
    cos = cos[:seq_len, :]
    sin = sin[:seq_len, :]

    # Reshape for broadcasting: [1, seq_len, 1, head_dim/2]
    cos = cos[None, :, None, :]
    sin = sin[None, :, None, :]

    # Apply rotation
    x_rotated_1 = x1 * cos - x2 * sin
    x_rotated_2 = x1 * sin + x2 * cos

    # Interleave back
    x_rotated = jnp.stack([x_rotated_1, x_rotated_2], axis=-1)
    x_rotated = x_rotated.reshape(x.shape)

    return x_rotated


class MultiQueryAttention(nn.Module):
    """
    Multi-Query Attention: Multiple Q heads, single K/V head.
    More efficient than standard MHA.
    """
    config: ModelConfig
    dtype: Any = jnp.float32

    @nn.compact
    def __call__(self, x, cos, sin, mask=None, deterministic=True):
        batch, seq_len, _ = x.shape
        config = self.config

        # Q projection: [batch, seq, num_heads * head_dim]
        q = nn.Dense(
            config.num_heads * config.head_dim,
            dtype=self.dtype,
            kernel_init=nn.initializers.normal(0.02),
            name='q_proj'
        )(x)

        # K, V projections: [batch, seq, head_dim] (single head)
        k = nn.Dense(
            config.head_dim,
            dtype=self.dtype,
            kernel_init=nn.initializers.normal(0.02),
            name='k_proj'
        )(x)
        v = nn.Dense(
            config.head_dim,
            dtype=self.dtype,
            kernel_init=nn.initializers.normal(0.02),
            name='v_proj'
        )(x)

        # Reshape Q: [batch, seq, num_heads, head_dim]
        q = q.reshape(batch, seq_len, config.num_heads, config.head_dim)

        # Reshape K, V: [batch, seq, 1, head_dim]
        k = k.reshape(batch, seq_len, 1, config.head_dim)
        v = v.reshape(batch, seq_len, 1, config.head_dim)

        # Apply RoPE to Q and K
        q = apply_rope(q, cos, sin)
        k = apply_rope(k, cos, sin)

        # Broadcast K, V to all heads: [batch, seq, num_heads, head_dim]
        k = jnp.broadcast_to(k, (batch, seq_len, config.num_heads, config.head_dim))
        v = jnp.broadcast_to(v, (batch, seq_len, config.num_heads, config.head_dim))

        # Transpose for attention: [batch, num_heads, seq, head_dim]
        q = q.transpose(0, 2, 1, 3)
        k = k.transpose(0, 2, 1, 3)
        v = v.transpose(0, 2, 1, 3)

        # Scaled dot-product attention
        scale = 1.0 / math.sqrt(config.head_dim)
        attn_weights = jnp.matmul(q, k.transpose(0, 1, 3, 2)) * scale

        # Apply causal mask
        if mask is not None:
            attn_weights = jnp.where(mask, attn_weights, -1e9)

        attn_weights = jax.nn.softmax(attn_weights.astype(jnp.float32), axis=-1).astype(self.dtype)

        # Apply attention to values
        attn_output = jnp.matmul(attn_weights, v)

        # Transpose back: [batch, seq, num_heads, head_dim]
        attn_output = attn_output.transpose(0, 2, 1, 3)

        # Reshape: [batch, seq, hidden_dim]
        attn_output = attn_output.reshape(batch, seq_len, config.num_heads * config.head_dim)

        # Output projection
        output = nn.Dense(
            config.hidden_dim,
            dtype=self.dtype,
            kernel_init=nn.initializers.normal(0.02),
            name='o_proj'
        )(attn_output)

        return output


class SwiGLUFeedForward(nn.Module):
    """SwiGLU Feed-Forward Network."""
    config: ModelConfig
    dtype: Any = jnp.float32

    @nn.compact
    def __call__(self, x, deterministic=True):
        config = self.config

        # Gate and up projections
        gate = nn.Dense(
            config.ffn_dim,
            dtype=self.dtype,
            kernel_init=nn.initializers.normal(0.02),
            name='gate_proj'
        )(x)
        up = nn.Dense(
            config.ffn_dim,
            dtype=self.dtype,
            kernel_init=nn.initializers.normal(0.02),
            name='up_proj'
        )(x)

        # SwiGLU: silu(gate) * up
        hidden = jax.nn.silu(gate) * up

        # Down projection
        output = nn.Dense(
            config.hidden_dim,
            dtype=self.dtype,
            kernel_init=nn.initializers.normal(0.02),
            name='down_proj'
        )(hidden)

        return output


class TransformerBlock(nn.Module):
    """Single transformer block with pre-norm architecture."""
    config: ModelConfig
    dtype: Any = jnp.float32

    @nn.compact
    def __call__(self, x, cos, sin, mask=None, deterministic=True):
        config = self.config

        # Pre-norm attention
        norm_x = RMSNorm(config.hidden_dim, dtype=self.dtype, name='attn_norm')(x)
        attn_out = MultiQueryAttention(config, dtype=self.dtype, name='attention')(
            norm_x, cos, sin, mask, deterministic
        )
        x = x + attn_out

        # Pre-norm FFN
        norm_x = RMSNorm(config.hidden_dim, dtype=self.dtype, name='ffn_norm')(x)
        ffn_out = SwiGLUFeedForward(config, dtype=self.dtype, name='ffn')(
            norm_x, deterministic
        )
        x = x + ffn_out

        return x


class PythonCodeModel(nn.Module):
    """
    Full decoder-only transformer for Python code generation.
    Uses tied embeddings (input and output share weights).
    """
    config: ModelConfig
    dtype: Any = jnp.float32

    def setup(self):
        config = self.config

        # Token embedding (will be tied with output)
        self.embed_tokens = nn.Embed(
            num_embeddings=config.vocab_size,
            features=config.hidden_dim,
            dtype=self.dtype,
            embedding_init=nn.initializers.normal(0.02),
            name='embed_tokens'
        )

        # Transformer layers
        self.layers = [
            TransformerBlock(config, dtype=self.dtype, name=f'layer_{i}')
            for i in range(config.num_layers)
        ]

        # Final normalization
        self.norm = RMSNorm(config.hidden_dim, dtype=self.dtype, name='final_norm')

        # Precompute RoPE embeddings
        self.cos, self.sin = create_rope_embeddings(
            config.head_dim,
            config.max_seq_len,
            config.rope_theta
        )

    def __call__(self, input_ids, deterministic=True):
        batch, seq_len = input_ids.shape
        config = self.config

        # Token embeddings
        x = self.embed_tokens(input_ids)

        # Create causal mask
        mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=bool))
        mask = mask[None, None, :, :]  # [1, 1, seq, seq]

        # Apply transformer layers
        for layer in self.layers:
            x = layer(x, self.cos, self.sin, mask, deterministic)

        # Final normalization
        x = self.norm(x)

        # Compute logits (tied with embedding weights)
        logits = x @ self.embed_tokens.embedding.T

        return logits


# ============ TRAINING UTILITIES ============
def count_parameters(params) -> int:
    """Count total parameters in the model."""
    return sum(p.size for p in jax.tree_util.tree_leaves(params))


def create_learning_rate_schedule(
    warmup_steps: int,
    total_steps: int,
    peak_lr: float,
    min_lr_ratio: float = 0.1
):
    """Create learning rate schedule with warmup and cosine decay."""
    def schedule(step):
        # Warmup phase
        warmup_factor = jnp.minimum(step / warmup_steps, 1.0)

        # Cosine decay phase
        progress = jnp.maximum(0.0, (step - warmup_steps) / (total_steps - warmup_steps))
        cosine_factor = 0.5 * (1.0 + jnp.cos(jnp.pi * progress))
        decay_factor = min_lr_ratio + (1 - min_lr_ratio) * cosine_factor

        return peak_lr * warmup_factor * decay_factor

    return schedule


def create_train_state(
    rng: jax.random.PRNGKey,
    model: PythonCodeModel,
    config: ModelConfig,
    training_config: TrainingConfig
) -> train_state.TrainState:
    """Initialize model parameters and optimizer."""

    # Initialize parameters
    dummy_input = jnp.ones((1, config.max_seq_len), dtype=jnp.int32)
    params = model.init(rng, dummy_input)

    # Count parameters
    num_params = count_parameters(params)
    print(f"\nModel parameters: {num_params:,}")

    # Create optimizer with schedule
    lr_schedule = create_learning_rate_schedule(
        training_config.warmup_steps,
        training_config.total_steps,
        training_config.learning_rate
    )

    optimizer = optax.chain(
        optax.clip_by_global_norm(training_config.grad_clip),
        optax.adamw(
            learning_rate=lr_schedule,
            b1=training_config.beta1,
            b2=training_config.beta2,
            weight_decay=training_config.weight_decay
        )
    )

    return train_state.TrainState.create(
        apply_fn=model.apply,
        params=params,
        tx=optimizer
    )


def compute_loss(logits, targets, pad_token_id):
    """
    Compute cross-entropy loss with padding mask.
    logits: [batch, seq_len, vocab_size]
    targets: [batch, seq_len]
    """
    # Shift for next-token prediction
    logits = logits[:, :-1, :]  # [batch, seq_len-1, vocab]
    targets = targets[:, 1:]    # [batch, seq_len-1]

    # Create padding mask
    mask = (targets != pad_token_id).astype(jnp.float32)

    # Compute cross-entropy
    log_probs = jax.nn.log_softmax(logits.astype(jnp.float32), axis=-1)
    target_log_probs = jnp.take_along_axis(
        log_probs,
        targets[:, :, None],
        axis=-1
    ).squeeze(-1)

    # Apply mask and compute mean
    masked_loss = -target_log_probs * mask
    loss = masked_loss.sum() / (mask.sum() + 1e-8)

    return loss


@partial(jax.jit, static_argnums=(3,))
def train_step(state, batch, rng, pad_token_id):
    """Single training step (JIT compiled)."""

    def loss_fn(params):
        logits = state.apply_fn(params, batch, deterministic=True)
        loss = compute_loss(logits, batch, pad_token_id)
        return loss

    loss, grads = jax.value_and_grad(loss_fn)(state.params)
    state = state.apply_gradients(grads=grads)

    return state, loss


# ============ GENERATION ============
def generate(
    state: train_state.TrainState,
    tokenizer,
    prompt: str,
    max_tokens: int = 50,
    temperature: float = 0.8,
    rng: jax.random.PRNGKey = None
) -> str:
    """
    Generate code completion using autoregressive sampling.
    """
    if rng is None:
        rng = jax.random.PRNGKey(42)

    # Tokenize prompt
    input_ids = tokenizer.encode(prompt, return_tensors='np')
    input_ids = jnp.array(input_ids)

    # Get config from model
    max_seq_len = 512  # From ModelConfig default

    for _ in range(max_tokens):
        # Truncate if needed
        if input_ids.shape[1] >= max_seq_len:
            break

        # Forward pass
        logits = state.apply_fn(state.params, input_ids, deterministic=True)

        # Get logits for last token
        next_token_logits = logits[0, -1, :]

        # Apply temperature
        next_token_logits = next_token_logits / temperature

        # Sample
        rng, sample_rng = jax.random.split(rng)
        next_token = jax.random.categorical(sample_rng, next_token_logits)

        # Check for EOS
        if next_token == tokenizer.eos_token_id:
            break

        # Append token
        input_ids = jnp.concatenate(
            [input_ids, next_token[None, None]],
            axis=1
        )

    # Decode
    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return generated_text


# ============ VALIDATION ============
def validate_pipeline(tokenizer, model_config: ModelConfig):
    """
    Run pre-flight checks before training.
    Returns True if all checks pass.
    """
    print("\n" + "="*50)
    print("PIPELINE VALIDATION")
    print("="*50 + "\n")

    all_passed = True

    # Check 1: Devices
    print("1. Checking devices...")
    devices = jax.devices()
    print(f"   JAX version: {jax.__version__}")
    print(f"   Devices: {devices}")
    print(f"   Device type: {DEVICE_TYPE}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Dtype: {DTYPE}")
    print("   [PASS] Devices available")

    # Check 2: Tokenizer
    print("\n2. Checking tokenizer...")
    test_code = "def hello():\n    print('Hello, World!')"
    tokens = tokenizer.encode(test_code)
    decoded = tokenizer.decode(tokens)
    print(f"   Vocab size: {tokenizer.vocab_size}")
    print(f"   Test encode length: {len(tokens)}")
    print(f"   Pad token ID: {tokenizer.pad_token_id}")
    print("   [PASS] Tokenizer works")

    # Check 3: Data loading
    print("\n3. Checking data quality filter...")
    data_config = DataConfig()
    filter_obj = StrictPythonFilter(data_config)

    # Test with good code
    good_code = '''
def calculate_fibonacci(n: int) -> int:
    """Calculate the nth Fibonacci number.

    Args:
        n: The position in the Fibonacci sequence.

    Returns:
        The nth Fibonacci number.
    """
    if n <= 1:
        return n
    return calculate_fibonacci(n - 1) + calculate_fibonacci(n - 2)
'''
    passes, score, _ = filter_obj.passes_all_filters(good_code)
    print(f"   Good code passes: {passes}, score: {score:.2f}")

    # Test with bad code (no structure)
    bad_code = "x = 1\ny = 2\nprint(x + y)"
    passes_bad, _, _ = filter_obj.passes_all_filters(bad_code)
    print(f"   Bad code (no structure) rejected: {not passes_bad}")

    print("   [PASS] Quality filter works")

    # Check 4: Model initialization
    print("\n4. Checking model initialization...")
    model = PythonCodeModel(model_config, dtype=DTYPE)
    rng = jax.random.PRNGKey(SEED)

    dummy_input = jnp.ones((1, 32), dtype=jnp.int32)
    params = model.init(rng, dummy_input)
    num_params = count_parameters(params)
    print(f"   Model parameters: {num_params:,}")

    expected_min = 9_000_000
    expected_max = 15_000_000
    if expected_min <= num_params <= expected_max:
        print(f"   [PASS] Parameter count in expected range ({expected_min:,} - {expected_max:,})")
    else:
        print(f"   [WARN] Parameter count outside expected range")
        all_passed = False

    # Check 5: Forward pass
    print("\n5. Checking forward pass...")
    test_input = jnp.ones((BATCH_SIZE, model_config.max_seq_len), dtype=jnp.int32)
    logits = model.apply(params, test_input, deterministic=True)
    expected_shape = (BATCH_SIZE, model_config.max_seq_len, model_config.vocab_size)
    print(f"   Input shape: {test_input.shape}")
    print(f"   Output shape: {logits.shape}")
    print(f"   Expected shape: {expected_shape}")

    if logits.shape == expected_shape:
        print("   [PASS] Output shape correct")
    else:
        print("   [FAIL] Output shape incorrect")
        all_passed = False

    # Check 6: Loss computation
    print("\n6. Checking loss computation...")
    loss = compute_loss(logits, test_input, tokenizer.pad_token_id)
    print(f"   Loss value: {loss:.4f}")

    if jnp.isfinite(loss):
        print("   [PASS] Loss is finite")
    else:
        print("   [FAIL] Loss is NaN or Inf")
        all_passed = False

    # Check 7: Training step
    print("\n7. Checking training step...")
    training_config = TrainingConfig(total_steps=100)
    state = create_train_state(rng, model, model_config, training_config)

    rng, step_rng = jax.random.split(rng)
    state, step_loss = train_step(state, test_input, step_rng, tokenizer.pad_token_id)
    print(f"   Step loss: {step_loss:.4f}")

    if jnp.isfinite(step_loss):
        print("   [PASS] Training step works")
    else:
        print("   [FAIL] Training step produces NaN")
        all_passed = False

    # Summary
    print("\n" + "="*50)
    if all_passed:
        print("ALL VALIDATION CHECKS PASSED")
    else:
        print("SOME CHECKS FAILED - Review above")
    print("="*50 + "\n")

    return all_passed


# ============ MAIN TRAINING ============
def train(
    tokenizer,
    model_config: ModelConfig,
    training_config: TrainingConfig,
    data_config: DataConfig,
    num_samples: int = 10000
) -> train_state.TrainState:
    """
    Main training loop.
    """
    print("\n" + "="*50)
    print("TRAINING")
    print("="*50)

    # Initialize model
    model = PythonCodeModel(model_config, dtype=DTYPE)
    rng = jax.random.PRNGKey(SEED)

    # Create train state
    state = create_train_state(rng, model, model_config, training_config)

    # Load data
    data_loader = QualityDataLoader(tokenizer, data_config, model_config)
    tokenized_samples, data_stats = data_loader.get_training_data(
        num_samples=num_samples,
        batch_size=BATCH_SIZE
    )

    # Training loop
    print(f"\nStarting training for {training_config.total_steps} steps...")
    print(f"Logging every {training_config.log_every} steps")

    losses = []
    step = 0

    pbar = tqdm(total=training_config.total_steps, desc="Training")

    while step < training_config.total_steps:
        # Create batch iterator for this epoch
        batch_iter = data_loader.create_batches(tokenized_samples, BATCH_SIZE)

        for batch in batch_iter:
            if step >= training_config.total_steps:
                break

            # Convert to JAX array
            batch = jnp.array(batch)

            # Training step
            rng, step_rng = jax.random.split(rng)
            state, loss = train_step(state, batch, step_rng, tokenizer.pad_token_id)

            losses.append(float(loss))
            step += 1

            # Update progress bar
            pbar.set_postfix({'loss': f'{loss:.4f}'})
            pbar.update(1)

            # Log
            if step % training_config.log_every == 0:
                avg_loss = sum(losses[-training_config.log_every:]) / training_config.log_every
                print(f"\nStep {step}, Loss: {avg_loss:.4f}")

    pbar.close()

    # Final stats
    print(f"\n{'='*50}")
    print("Training complete!")
    print(f"Final loss: {losses[-1]:.4f}")
    print(f"Average loss (last 100): {sum(losses[-100:])/min(len(losses), 100):.4f}")
    print(f"{'='*50}")

    return state


# ============ ENTRY POINT ============
def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Train 10M Python code model')
    parser.add_argument('--validate-only', action='store_true', help='Only run validation')
    parser.add_argument('--steps', type=int, default=None, help='Number of training steps')
    parser.add_argument('--samples', type=int, default=10000, help='Number of training samples')
    args = parser.parse_args()

    # Print header
    print("\n" + "="*60)
    print("10M Parameter Python Code Model - Pipeline Validation")
    print("="*60)
    print(f"\nDevice: {DEVICE_TYPE}")
    print(f"Batch size: {BATCH_SIZE}")
    print(f"Dtype: {DTYPE}")

    # Load tokenizer
    print("\nLoading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("bigcode/starcoder2-3b")
    tokenizer.pad_token = tokenizer.eos_token
    print(f"Tokenizer loaded: vocab_size={tokenizer.vocab_size}")

    # Create configs
    model_config = ModelConfig()
    training_config = TrainingConfig()
    data_config = DataConfig()

    # Override steps if provided
    if args.steps is not None:
        training_config.total_steps = args.steps
    elif DEVICE_TYPE == 'CPU':
        # Shorter training on CPU
        training_config.total_steps = 500
        training_config.log_every = 50

    # Run validation
    if not validate_pipeline(tokenizer, model_config):
        print("\nValidation failed! Please fix issues before training.")
        return

    if args.validate_only:
        print("\nValidation complete (--validate-only specified)")
        return

    # Train
    state = train(
        tokenizer,
        model_config,
        training_config,
        data_config,
        num_samples=args.samples
    )

    # Test generation
    print("\n" + "="*50)
    print("GENERATION TEST")
    print("="*50)

    prompts = [
        "def fibonacci(n):",
        "class DataProcessor:",
        "def calculate_mean(numbers: list) -> float:",
    ]

    rng = jax.random.PRNGKey(42)
    for prompt in prompts:
        print(f"\nPrompt: {prompt}")
        rng, gen_rng = jax.random.split(rng)
        generated = generate(state, tokenizer, prompt, max_tokens=50, rng=gen_rng)
        print(f"Generated:\n{generated}")
        print("-" * 40)

    print("\nPipeline validation complete!")


if __name__ == "__main__":
    main()
