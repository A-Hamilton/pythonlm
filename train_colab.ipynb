{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PythonCoder v2 - Diff-Llama-MTP Training\n",
        "\n",
        "**Architecture**: Differential Attention + GQA + Multi-Token Prediction\n",
        "\n",
        "## Model (1B Parameters)\n",
        "| Feature | Config | Benchmark Gain |\n",
        "|---------|--------|----------------|\n",
        "| **Differential Attention** | Head-pairing | +35-40% efficiency |\n",
        "| **MTP (4 tokens)** | Enabled | +12-17% HumanEval |\n",
        "| **HLP** | Enabled | +24% FIM accuracy |\n",
        "| **YaRN** | 4x | Free context extension |\n",
        "| **Context** | 8192 | 2026 minimum |\n",
        "\n",
        "## Requirements\n",
        "- **Colab Pro+** with **TPU v6e-1** runtime (32GB HBM)\n",
        "- Google Drive with preprocessed data (~44GB)\n",
        "\n",
        "## What Changed (v2)\n",
        "- **Removed MoE**: Not worth it at <1B params\n",
        "- **Removed MLA**: GQA sufficient at 1B scale\n",
        "- **Removed Mamba**: Untested ROI for code\n",
        "- **Code**: 3002 → 979 lines (-67%)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Dependencies\n",
        "# ============================================================================\n",
        "# IMPORTANT: Run this cell FIRST, then restart runtime before continuing!\n",
        "# ============================================================================\n",
        "\n",
        "# Uninstall existing JAX to avoid conflicts\n",
        "!pip uninstall -y jax jaxlib flax optax orbax-checkpoint 2>/dev/null\n",
        "\n",
        "# Install JAX with TPU support\n",
        "!pip install -q \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "\n",
        "# Install Flax NNX + Optax + Orbax\n",
        "!pip install -q \"flax>=0.12.0\" \"optax>=0.2.0\" \"orbax-checkpoint>=0.6.0\"\n",
        "\n",
        "# Data loading\n",
        "!pip install -q \"transformers>=4.40.0\" \"grain>=0.2.0\"\n",
        "!pip install -q \"pyarrow>=14.0.0\" \"array-record>=0.6.0\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESTART RUNTIME NOW!\")\n",
        "print(\"Runtime -> Restart runtime, then run Cell 2\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Mount Google Drive & Setup\n",
        "# ============================================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# Project paths\n",
        "PROJECT_DIR = '/content/drive/MyDrive/python-coder-v6e'\n",
        "DATA_DIR = f'{PROJECT_DIR}/preprocessed_data/train'\n",
        "CHECKPOINT_DIR = f'{PROJECT_DIR}/checkpoints_v2'  # v2 checkpoints\n",
        "JAX_CACHE_DIR = f'{PROJECT_DIR}/jax_cache'\n",
        "\n",
        "# Create directories\n",
        "for d in [CHECKPOINT_DIR, JAX_CACHE_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# Configure JAX cache\n",
        "os.environ[\"JAX_COMPILATION_CACHE_DIR\"] = JAX_CACHE_DIR\n",
        "\n",
        "print(f\"Project: {PROJECT_DIR}\")\n",
        "print(f\"Data: {DATA_DIR}\")\n",
        "print(f\"Checkpoints: {CHECKPOINT_DIR}\")\n",
        "print(f\"JAX Cache: {JAX_CACHE_DIR}\")"
      ],
      "metadata": {
        "id": "mount"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Copy v2 Files from Drive\n",
        "# ============================================================================\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Copy v2 files (new architecture)\n",
        "v2_files = ['model_v2.py', 'train_v2.py']\n",
        "for f in v2_files:\n",
        "    src = f'{PROJECT_DIR}/{f}'\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, f'./{f}')\n",
        "        print(f\"✓ Copied {f}\")\n",
        "    else:\n",
        "        print(f\"✗ ERROR: {f} not found!\")\n",
        "\n",
        "# Copy other required files\n",
        "other_files = ['inference.py', 'preprocess_data.py']\n",
        "for f in other_files:\n",
        "    src = f'{PROJECT_DIR}/{f}'\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, f'./{f}')\n",
        "        print(f\"✓ Copied {f}\")\n",
        "\n",
        "# Copy tokenizer\n",
        "tokenizer_src = f'{PROJECT_DIR}/qwen_tokenizer'\n",
        "if os.path.exists(tokenizer_src):\n",
        "    if os.path.exists('./qwen_tokenizer'):\n",
        "        shutil.rmtree('./qwen_tokenizer')\n",
        "    shutil.copytree(tokenizer_src, './qwen_tokenizer')\n",
        "    print(\"✓ Copied qwen_tokenizer/\")\n",
        "else:\n",
        "    print(\"✗ ERROR: qwen_tokenizer not found!\")\n",
        "\n",
        "print(\"\\nLocal files:\")\n",
        "!ls -la *.py 2>/dev/null | grep -E \"model_v2|train_v2\""
      ],
      "metadata": {
        "id": "copy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Verify Setup\n",
        "# ============================================================================\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SETUP VERIFICATION (v2 Architecture)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check v2 files\n",
        "print(\"\\n[v2 Files]\")\n",
        "v2_ok = True\n",
        "for f in ['model_v2.py', 'train_v2.py']:\n",
        "    if os.path.exists(f):\n",
        "        lines = len(open(f).readlines())\n",
        "        print(f\"  ✓ {f} ({lines} lines)\")\n",
        "    else:\n",
        "        print(f\"  ✗ {f} MISSING\")\n",
        "        v2_ok = False\n",
        "\n",
        "# Check tokenizer\n",
        "print(\"\\n[Tokenizer]\")\n",
        "if os.path.exists('./qwen_tokenizer/tokenizer.json'):\n",
        "    print(\"  ✓ qwen_tokenizer/ present\")\n",
        "else:\n",
        "    print(\"  ✗ Tokenizer missing!\")\n",
        "    v2_ok = False\n",
        "\n",
        "# Check training data\n",
        "print(\"\\n[Training Data]\")\n",
        "parquet_files = glob.glob(f'{DATA_DIR}/*.parquet')\n",
        "if parquet_files:\n",
        "    total_size = sum(os.path.getsize(f) for f in parquet_files) / (1024**3)\n",
        "    print(f\"  ✓ {len(parquet_files)} shards ({total_size:.1f} GB)\")\n",
        "else:\n",
        "    print(f\"  ✗ No data in {DATA_DIR}\")\n",
        "    v2_ok = False\n",
        "\n",
        "# Check existing checkpoints\n",
        "print(\"\\n[v2 Checkpoints]\")\n",
        "ckpts = glob.glob(f'{CHECKPOINT_DIR}/epoch_*')\n",
        "if ckpts:\n",
        "    latest = max(ckpts, key=lambda p: int(os.path.basename(p).split('_')[1]))\n",
        "    print(f\"  Found {len(ckpts)} checkpoints\")\n",
        "    print(f\"  Latest: {os.path.basename(latest)}\")\n",
        "    print(\"  → Training will auto-resume\")\n",
        "else:\n",
        "    print(\"  No checkpoints (fresh training)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if v2_ok:\n",
        "    print(\"✓ READY FOR v2 TRAINING\")\n",
        "    print(\"\\nArchitecture: Diff-Llama-MTP (1B params)\")\n",
        "    print(\"Features: DiffAttn + MTP + HLP + YaRN\")\n",
        "else:\n",
        "    print(\"✗ FIX ERRORS ABOVE\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "verify"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Start Training (v2)\n",
        "# ============================================================================\n",
        "# Trains 1B Diff-Llama-MTP model\n",
        "#\n",
        "# Architecture (v2):\n",
        "#   - Differential Attention + GQA (35-40% efficiency)\n",
        "#   - Multi-Token Prediction (+12-17% HumanEval)\n",
        "#   - Horizon Length Prediction (+24% FIM)\n",
        "#   - YaRN (4x context extension)\n",
        "#   - Dense FFN (no MoE - not worth it at 1B)\n",
        "#\n",
        "# Config:\n",
        "#   - 1B parameters\n",
        "#   - 8192 context length\n",
        "#   - Batch: 2 x 32 = 64 effective\n",
        "#   - LR: 3e-4 with warmup-cosine\n",
        "#   - Auto-resume from checkpoint\n",
        "# ============================================================================\n",
        "\n",
        "!python train_v2.py"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Check Training Progress\n",
        "# ============================================================================\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING PROGRESS (v2)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "ckpts = sorted(glob.glob(f'{CHECKPOINT_DIR}/epoch_*'),\n",
        "               key=lambda p: int(os.path.basename(p).split('_')[1]))\n",
        "\n",
        "if ckpts:\n",
        "    print(f\"\\nCheckpoints: {len(ckpts)}\")\n",
        "    print(\"\\nEpoch | Size\")\n",
        "    print(\"-\" * 20)\n",
        "    for ckpt in ckpts:\n",
        "        epoch = os.path.basename(ckpt).split('_')[1]\n",
        "        size_mb = sum(os.path.getsize(os.path.join(ckpt, f))\n",
        "                     for f in os.listdir(ckpt) if os.path.isfile(os.path.join(ckpt, f))) / (1024**2)\n",
        "        print(f\"  {epoch:3s}  | {size_mb:.0f} MB\")\n",
        "else:\n",
        "    print(\"\\nNo checkpoints yet.\")\n",
        "\n",
        "# JAX cache\n",
        "cache_files = glob.glob(f'{JAX_CACHE_DIR}/*')\n",
        "if cache_files:\n",
        "    cache_size = sum(os.path.getsize(f) for f in cache_files if os.path.isfile(f)) / (1024**2)\n",
        "    print(f\"\\nJAX cache: {len(cache_files)} files ({cache_size:.0f} MB)\")"
      ],
      "metadata": {
        "id": "progress"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Quick Inference Test\n",
        "# ============================================================================\n",
        "# Test the trained model\n",
        "# WARNING: This initializes TPU - may conflict with training\n",
        "# ============================================================================\n",
        "\n",
        "RUN_TEST = False  # Set to True to test\n",
        "\n",
        "if RUN_TEST:\n",
        "    test_code = '''\n",
        "import sys\n",
        "sys.path.insert(0, \".\")\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "from model_v2 import create_model, CONFIG_1B\n",
        "\n",
        "# Create model\n",
        "print(\"Loading model...\")\n",
        "model = create_model(CONFIG_1B)\n",
        "\n",
        "# Simple test\n",
        "print(\"Testing forward pass...\")\n",
        "input_ids = jnp.ones((1, 64), dtype=jnp.int32)\n",
        "out = model(input_ids)\n",
        "print(f\"Output shape: {out['logits'].shape}\")\n",
        "print(\"✓ Model works!\")\n",
        "'''\n",
        "    with open('/tmp/test_v2.py', 'w') as f:\n",
        "        f.write(test_code)\n",
        "    !python /tmp/test_v2.py\n",
        "else:\n",
        "    print(\"Test disabled. Set RUN_TEST = True to run.\")"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### TPU Not Found\n",
        "1. Runtime → Change runtime type → TPU\n",
        "2. Restart runtime, re-run from Cell 2\n",
        "\n",
        "### Out of Memory\n",
        "- v2 model uses ~4GB for 1B params\n",
        "- TPU v6e-1 has 32GB - plenty of room\n",
        "- If OOM, reduce `micro_batch_size` in train_v2.py\n",
        "\n",
        "### Session Timeout\n",
        "- Training auto-resumes from latest checkpoint\n",
        "- Re-run Cells 2-5\n",
        "\n",
        "### Slow First Step\n",
        "- JAX compiles the graph (cached for next time)\n",
        "- First epoch may be 2-3x slower\n",
        "\n",
        "## Architecture Notes\n",
        "\n",
        "### v2 vs v1\n",
        "| Component | v1 | v2 | Why |\n",
        "|-----------|----|----|-----|\n",
        "| FFN | MoE 32×4 | Dense | MoE needs 5T+ tokens |\n",
        "| Attention | MLA | GQA | MLA overkill for 1B |\n",
        "| Hybrid | Mamba | None | Untested ROI |\n",
        "| Code | 3002 lines | 979 lines | -67% |\n",
        "\n",
        "### Benchmark Justifications\n",
        "- **Differential Attention**: 35-40% efficiency ([Microsoft](https://arxiv.org/abs/2410.05258))\n",
        "- **MTP**: +12-17% HumanEval ([Meta](https://arxiv.org/pdf/2404.19737))\n",
        "- **HLP**: +24% FIM accuracy\n",
        "- **YaRN**: Free context extension"
      ],
      "metadata": {
        "id": "troubleshooting"
      }
    }
  ]
}
